---
layout: homepage
---

<!-- ÂØºËà™Ê†áÁ≠æÈ°µ -->
<nav class="page-tabs">
  <button class="tab-button active" data-tab="about">About Me</button>
  <button class="tab-button" data-tab="professional">Professional & Academic Services</button>
  <button class="tab-button" data-tab="gallery">Gallery</button>
</nav>

<!-- About Me ÂÜÖÂÆπ -->
<div id="about-tab" class="tab-content active">

<div class="two-column-layout">
<div class="left-column" markdown="1">

üëã I am a **Ph.D. candidate** in the AI Thrust at HKUST(GZ). I am fortunate to be advised by [Prof. Xuming Hu @ HKUST](https://xuminghu.github.io/) and [Prof. Raymond Chi-Wing Wong @ HKUST](https://www.cse.ust.hk/~raywong/). I had served as a **Resident Doctoral Researcher** at [INSAIT](https://insait.ai/) supervised by [Prof. Luc Van Gool](https://insait.ai/prof-luc-van-gool/) from 2025.02 to 2026.02. Recently, I have also been collaborating with [Prof. Philip S. Yu @ UIC](https://scholar.google.com.hk/citations?user=D0lL1r0AAAAJ&hl=zh-CN&oi=ao), [Prof. Nicu Sebe @ UNITN](https://disi.unitn.it/~sebe/), [Linfeng Zhang @ SJTU](http://www.zhanglinfeng.tech/), and [Kailun Yang @ HNU](https://www.yangkailun.com/). 

My doctoral research develops robust and interpretable multi-modal learning algorithms spanning **perception**, **understanding**, **reasoning**, and **generation**. My two main doctoral research directions are:

<details class="research-direction">
  <summary><strong>Omnidirectional Vision</strong> <span class="expand-hint">(click to expand)</span></summary>
  <div class="research-timeline-container">
    <div class="research-timeline-item">
      <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zheng_Both_Style_and_Distortion_Matter_Dual-Path_Unsupervised_Domain_Adaptation_for_CVPR_2023_paper.pdf"><strong>DPPASS</strong></a>
      <span class="research-venue">CVPR 2023</span>
    </div>
    <span class="timeline-arrow">‚Üí</span>
    <div class="research-timeline-item">
      <a href="http://openaccess.thecvf.com/content/ICCV2023/papers/Zheng_Look_at_the_Neighbor_Distortion-aware_Unsupervised_Domain_Adaptation_for_Panoramic_ICCV_2023_paper.pdf"><strong>DATR</strong></a>
      <span class="research-venue">ICCV 2023</span>
    </div>
    <span class="timeline-arrow">‚Üí</span>
    <div class="research-timeline-item">
      <a href="http://openaccess.thecvf.com/content/CVPR2024/papers/Zheng_Semantics_Distortion_and_Style_Matter_Towards_Source-free_UDA_for_Panoramic_CVPR_2024_paper.pdf"><strong>360SFUDA</strong></a>
      <span class="research-venue">CVPR 2024</span>
    </div>
    <!-- <span class="timeline-arrow">‚Üí</span>
    <div class="research-timeline-item">
      <a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Zhang_GoodSAM_Bridging_Domain_and_Capacity_Gaps_via_Segment_Anything_Model_CVPR_2024_paper.pdf"><strong>GoodSAM</strong></a>
      <span class="research-venue">CVPR 2024</span>
    </div> -->
    <span class="timeline-arrow">‚Üí</span>
    <div class="research-timeline-item">
      <a href="https://dl.acm.org/doi/abs/10.1109/TPAMI.2024.3490619"><strong>360SFUDA++</strong></a>
      <span class="research-venue">TPAMI 2025</span>
    </div>
    <span class="timeline-arrow">‚Üí</span>
    <div class="research-timeline-item">
      <a href="https://openaccess.thecvf.com/content/ICCV2025/papers/Zhong_OmniSAM_Omnidirectional_Segment_Anything_Model_for_UDA_in_Panoramic_Semantic_ICCV_2025_paper.pdf"><strong>OmniSAM</strong></a>
      <span class="research-venue">ICCV 2025 <span class="highlight-badge">Highlight</span></span>
    </div>
    <span class="timeline-arrow">‚Üí</span>
    <div class="research-timeline-item">
      <a href="https://arxiv.org/pdf/2506.21198"><strong>UNLOCK</strong></a>
      <span class="research-venue">ICCV 2025</span>
    </div>
    <span class="timeline-arrow">‚Üí</span>
    <div class="research-timeline-item">
      <a href="https://dl.acm.org/doi/pdf/10.1145/3743093.3770977"><strong>Pano-R1</strong></a>
      <span class="research-venue">ACM MM Asia 2025</span>
    </div>
    <span class="timeline-arrow">‚Üí</span>
    <div class="research-timeline-item">
      <a href="https://arxiv.org/pdf/2505.11907"><strong>OSR-Bench</strong></a>
      <span class="research-venue">CVPR 2026</span>
    </div>
  </div>
</details>

<details class="research-direction">
  <summary><strong>Multi-modal Visual Understanding</strong> <span class="expand-hint">(click to expand)</span></summary>
  <div class="research-timeline-container">
    <div class="research-timeline-item">
      <a href="http://openaccess.thecvf.com/content/CVPR2024/papers/Zhou_ExACT_Language-guided_Conceptual_Reasoning_and_Uncertainty_Estimation_for_Event-based_Action_CVPR_2024_paper.pdf"><strong>ExACT</strong></a>
      <span class="research-venue">CVPR 2024 <span class="highlight-badge">Highlight</span></span>
    </div>
    <!-- <span class="timeline-arrow">‚Üí</span>
    <div class="research-timeline-item">
      <a href="http://openaccess.thecvf.com/content/CVPR2024/papers/Lyu_UniBind_LLM-Augmented_Unified_and_Balanced_Representation_Space_to_Bind_Them_CVPR_2024_paper.pdf"><strong>UniBind</strong></a>
      <span class="research-venue">CVPR 2024</span>
    </div> -->
    <span class="timeline-arrow">‚Üí</span>
    <div class="research-timeline-item">
      <a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Zheng_EventDance_Unsupervised_Source-free_Cross-modal_Adaptation_for_Event-based_Object_Recognition_CVPR_2024_paper.pdf"><strong>EventDance</strong></a>
      <span class="research-venue">CVPR 2024</span>
    </div>
    <!-- <span class="timeline-arrow">‚Üí</span>
    <div class="research-timeline-item">
      <a href="https://arxiv.org/pdf/2309.09297"><strong>EOLO</strong></a>
      <span class="research-venue">ICRA 2024</span>
    </div> -->
    <span class="timeline-arrow">‚Üí</span>
    <div class="research-timeline-item">
      <a href="https://arxiv.org/pdf/2308.03135"><strong>EventBind</strong></a>
      <span class="research-venue">ECCV 2024</span>
    </div>
    <span class="timeline-arrow">‚Üí</span>
    <div class="research-timeline-item">
      <a href="https://arxiv.org/pdf/2407.11344"><strong>MAGIC</strong></a>
      <span class="research-venue">ECCV 2024</span>
    </div>
    <span class="timeline-arrow">‚Üí</span>
    <div class="research-timeline-item">
      <a href="https://arxiv.org/pdf/2407.11351"><strong>Any2Seg</strong></a>
      <span class="research-venue">ECCV 2024 <span class="highlight-badge">Oral</span></span>
    </div>
    <span class="timeline-arrow">‚Üí</span>
    <div class="research-timeline-item">
      <a href="https://openaccess.thecvf.com/content/CVPR2025W/TMM-OpenWorld/papers/Liao_Benchmarking_Multi-modal_Semantic_Segmentation_under_Sensor_Failures_Missing_and_Noisy_CVPRW_2025_paper.pdf"><strong>MMSS-Bench</strong></a>
      <span class="research-venue">CVPRW 2025 <span class="highlight-badge">Best Paper</span></span>
    </div>
    <!-- <span class="timeline-arrow">‚Üí</span>
    <div class="research-timeline-item">
      <a href="https://arxiv.org/pdf/2503.02581"><strong>SHIFTNet</strong></a>
      <span class="research-venue">IROS 2025</span>
    </div> -->
    <span class="timeline-arrow">‚Üí</span>
    <div class="research-timeline-item">
      <a href="https://arxiv.org/pdf/2505.06635"><strong>MFEnR</strong></a>
      <span class="research-venue">ICCV 2025</span>
    </div>
    <span class="timeline-arrow">‚Üí</span>
    <div class="research-timeline-item">
      <a href="https://arxiv.org/pdf/2505.12861"><strong>HPD</strong></a>
      <span class="research-venue">CVPR 2026</span>
    </div>
  </div>
</details>

My recent research interest lies in:

<div class="research-table">
  <div class="research-row">
    <div class="research-topic"><strong>Artificial Intelligence Generated Content (AIGC)</strong></div>
    <div class="research-works">
      <div class="research-timeline-item">
        <a href="https://arxiv.org/pdf/2502.00848"><strong>RealRAG</strong></a>
        <span class="research-venue">ICML 2025</span>
      </div>
      <span class="timeline-arrow">‚Üí</span>
      <div class="research-timeline-item">
        <a href="https://arxiv.org/pdf/2506.09482"><strong>TransDiff</strong></a>
        <span class="research-venue">arXiv 2025</span>
      </div>
    </div>
  </div>

  <div class="research-row">
    <div class="research-topic"><strong>Multimodal Foundation Models</strong></div>
    <div class="research-works">
      <div class="research-timeline-item">
        <a href="http://openaccess.thecvf.com/content/CVPR2024/papers/Lyu_UniBind_LLM-Augmented_Unified_and_Balanced_Representation_Space_to_Bind_Them_CVPR_2024_paper.pdf"><strong>UniBind</strong></a>
        <span class="research-venue">CVPR 2024</span>
      </div>
      <span class="timeline-arrow">‚Üí</span>
      <div class="research-timeline-item">
        <a href="https://arxiv.org/pdf/2509.18639?"><strong>UiG</strong></a>
        <span class="research-venue">Arxiv 2025</span>
      </div>
    </div>
  </div>

  <div class="research-row">
    <div class="research-topic"><strong>Scene Understanding & Spatial Intelligence</strong></div>
    <div class="research-works">
      <div class="research-timeline-item">
        <a href="https://dl.acm.org/doi/pdf/10.1145/3743093.3770977"><strong>Pano-R1</strong></a>
        <span class="research-venue">ACM MM Asia 2025</span>
      </div>
      <span class="timeline-arrow">‚Üí</span>
      <div class="research-timeline-item">
        <a href="https://arxiv.org/pdf/2510.06218"><strong>Egonight</strong></a>
        <span class="research-venue">ICLR 2026</span>
      </div>
    </div>
  </div>

  <div class="research-row">
    <div class="research-topic"><strong>Novel / Omnidirectional Sensors</strong></div>
    <div class="research-works">
      <div class="research-timeline-item">
        <a href="https://arxiv.org/pdf/2404.16501"><strong>360SFUDA++</strong></a>
        <span class="research-venue">TPAMI 2024</span>
      </div>
      <span class="timeline-arrow">‚Üí</span>
      <div class="research-timeline-item">
        <a href="https://openaccess.thecvf.com/content/ICCV2025/papers/Zhong_OmniSAM_Omnidirectional_Segment_Anything_Model_for_UDA_in_Panoramic_Semantic_ICCV_2025_paper.pdf"><strong>OmniSAM</strong></a>
        <span class="research-venue">ICCV 2025</span>
      </div>
    </div>
  </div>

  <div class="research-row">
    <div class="research-topic"><strong>Robustness & Security</strong></div>
    <div class="research-works">
      <div class="research-timeline-item">
        <a href="https://openaccess.thecvf.com/content/ICCV2025/papers/Lu_CIARD_Cyclic_Iterative_Adversarial_Robustness_Distillation_ICCV_2025_paper.pdf"><strong>CIARD</strong></a>
        <span class="research-venue">ICCV 2025</span>
      </div>
      <span class="timeline-arrow">‚Üí</span>
      <div class="research-timeline-item">
        <a href="https://arxiv.org/pdf/2511.21574"><strong>MRPD</strong></a>
        <span class="research-venue">AAAI 2026</span>
      </div>
    </div>
  </div>
</div>

I also survey papers in cutting-edge topics:

<div class="research-table">
  <div class="research-row">
    <div class="research-topic"><strong>Survey Projects</strong></div>
    <div class="research-works">
      <div class="survey-item">
        <a href="https://github.com/zhengxuJosh/Awesome-RAG-Vision"><strong>RAG for Computer Vision</strong></a>
      </div>
      <span class="timeline-arrow">‚Ä¢</span>
      <div class="survey-item">
        <a href="https://github.com/zhengxuJosh/Awesome-Multimodal-Spatial-Reasoning"><strong>Multi-modal Spatial Reasoning</strong></a>
      </div>
      <span class="timeline-arrow">‚Ä¢</span>
      <div class="survey-item">
        <a href="https://github.com/Chenfei-Liao/Awesome-360-Vision-Embodied-AI"><strong>360 Vision in Embodied AI</strong></a>
      </div>
    </div>
  </div>
</div>

</div>

<div class="right-column" markdown="1">

<div class="job-seeking-banner">
üî• I am actively seeking job opportunities (academia & industry) for Fall 2026!
</div>

## News

<div class="news-container">
{% capture news_content %}{% include_relative _includes/news.md %}{% endcapture %}
{{ news_content | markdownify }}
</div>

</div>
</div>

</div>

<!-- Professional & Academic Services ÂÜÖÂÆπ -->
<div id="professional-tab" class="tab-content" markdown="1">

### Invited Talks

<div class="service-card" markdown="1">

- **"Omnidirectional Vision: From Scene Understanding, Spatial Intelligence to Industrial Applications"**  
  *SPIC Energy Science and Technology Research Institute*, Shanghai, China, August 2025

- **"PANORAMA: Exploring the Industrial Potentials of Omnidirectional Vision"**  
  *Yangtze River Delta International Talent Port*, Wuxi, China, August 2025

- **"Retrieval-augmented Realistic Image Generation via Self-reflective Contrastive Learning"**  
  *VIVO*, Shenzhen, China, August 2025. Invited by [Dr. Kanzhi Wu](https://scholar.google.com.hk/citations?user=N0WHQ2wAAAAJ&hl=zh-CN&oi=ao)

</div>

### Mentorship

<div class="service-card" markdown="1">

**Current:** 
[Chenfei Liao (MPhil, HKUST-GZ)](https://chenfei-liao.github.io/); [Zihao Dongfang (RA, HKUST-GZ)](https://scholar.google.com.hk/citations?hl=zh-CN&user=IvJ4_xsAAAAJ); [Ziqiao Weng (MPhil, HKUST-GZ)](https://katie312.github.io/)

**Past:** [Yuanhuiyi Lyu (PhD, HKUST-GZ)](https://qc-ly.github.io/); [Lutao Jiang (PhD, HKUST-GZ)](https://lutao2021.github.io/); [Jialei Chen (PhD, Nagoya)](https://psmobile.github.io/); Mengzhen Chi (PhD, NEU); Junha Moon (MPhil, HKUST-GZ); Kaiyu Lei (MPhil, HKUST-GZ); Leyi Sheng (UG, HKUST-GZ); Ding Zhong (MS, Michigan); Yunhao Luo (PhD, Umich); Tianbo Pan (PhD, NUS); Zhenquan Zhang (MPhil, SCUT); [Boyuan Zheng (MPhil, Tongji)](https://nathandrake67.github.io/zhengby.github.io/)

‚úâÔ∏è <strong>Feel free to contact me for discussion and collaboration!</strong>

</div>

### Academic Services

<div class="service-card" markdown="1">

{% include_relative _includes/services.md %}

</div>

</div>

<!-- Gallery ÂÜÖÂÆπ -->
<div id="gallery-tab" class="tab-content" markdown="1">

{% include_relative _includes/gallery.md %}

</div>

<!-- Ê†áÁ≠æÈ°µÂàáÊç¢JavaScript -->
<script>
document.addEventListener('DOMContentLoaded', function() {
  const tabButtons = document.querySelectorAll('.tab-button');
  const tabContents = document.querySelectorAll('.tab-content');
  
  tabButtons.forEach(button => {
    button.addEventListener('click', function() {
      const targetTab = this.getAttribute('data-tab');
      
      // ÁßªÈô§ÊâÄÊúâactiveÁ±ª
      tabButtons.forEach(btn => btn.classList.remove('active'));
      tabContents.forEach(content => content.classList.remove('active'));
      
      // Ê∑ªÂä†activeÁ±ªÂà∞ÂΩìÂâçÊ†áÁ≠æ
      this.classList.add('active');
      document.getElementById(targetTab + '-tab').classList.add('active');
    });
  });
});
</script>
