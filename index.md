---
layout: homepage
---

### About Me

üëã I am a **Ph.D. candidate** in the AI Thrust at The Hong Kong University of Science and Technology, Guangzhou campus. I am fortunate to be advised by [Prof. Xuming Hu @ HKUST](https://xuminghu.github.io/) and [Prof. Raymond Chi-Wing Wong @ HKUST](https://www.cse.ust.hk/~raywong/). I also serve as a **Resident Doctoral Researcher** at [INSAIT](https://insait.ai/), under the supervision of [Prof. Luc Van Gool](https://insait.ai/prof-luc-van-gool/) and [Dr. Danda Paudel](https://insait.ai/dr-danda-paudel/). 

Recently, I have also been collaborating with [Prof. Nicu Sebe @ UNITN](https://disi.unitn.it/~sebe/), [Linfeng Zhang @ SJTU](http://www.zhanglinfeng.tech/), and [Kailun Yang @ HNU](https://www.yangkailun.com/). 

My doctoral research focuses on developing algorithms for robust and interpretable multi-modal learning that span the full spectrum of **perception**, **understanding**, **reasoning**, and **generation**.

Currently, I focus on:

**Artificial Intelligence Generated Content (AIGC)**: 
**[RealRAG](https://arxiv.org/pdf/2502.00848)** (ICML 2025): Applying retrieval-augmented generation (RAG) for realistic image generation.
**[TransDiff](https://arxiv.org/pdf/2506.09482)** (arXiv 2025): Marrying autoregressive transformers with diffusion models for fast image generation.

**Multimodal Foundation Models**: **[UniBind](http://openaccess.thecvf.com/content/CVPR2024/papers/Lyu_UniBind_LLM-Augmented_Unified_and_Balanced_Representation_Space_to_Bind_Them_CVPR_2024_paper.pdf)** (CVPR 2024): Learning modality-agnostic alignment centers for unified multimodal representation. **[EventBind](https://arxiv.org/pdf/2308.03135)** (ECCV 2024): Unleashing the potential of vision-language models for event-based recognition.

**Scene Understanding & Spatial Reasoning**: **[OmniSAM](https://openaccess.thecvf.com/content/ICCV2025/papers/Zhong_OmniSAM_Omnidirectional_Segment_Anything_Model_for_UDA_in_Panoramic_Semantic_ICCV_2025_paper.pdf)** (ICCV 2025): Adapting Segment Anything Model (SAM) for omnidirectional vision. **[OSR-Bench](https://arxiv.org/pdf/2505.11907)** (arXiv 2025): Benchmarking multimodal large language models for omnidirectional spatial reasoning.

**Novel / Omnidirectional Sensors**: **[360SFUDA++](https://arxiv.org/pdf/2404.16501)** (TPAMI 2024): Source-free unsupervised domain adaptation from pinhole images to panoramas.
**[DPPASS](https://openaccess.thecvf.com/content/CVPR2023/papers/Zheng_Both_Style_and_Distortion_Matter_Dual-Path_Unsupervised_Domain_Adaptation_for_CVPR_2023_paper.pdf)** (CVPR 2023): Dual-path unsupervised domain adaptation for panoramic segmentation.

**Robustness & Knowledge Distillation**: **[CIARD](https://openaccess.thecvf.com/content/ICCV2025/papers/Lu_CIARD_Cyclic_Iterative_Adversarial_Robustness_Distillation_ICCV_2025_paper.pdf)** (ICCV 2025): Cyclic iterative adversarial robustness distillation. **[C2VKD](https://arxiv.org/pdf/2310.07265)** (Pattern Recognition 2024): Distilling efficient vision transformers from CNNs for semantic segmentation.

<span style="color:red; font-weight:bold">üî• I am actively seeking job opportunities (academia & industry) for Fall 2026!</span>

---

### News

{% include_relative _includes/news.md %}

---

### Invited Talks

- **"Omnidirectional Vision: From Scene Understanding, Spatial Intelligence to Industrial Applications"**  
  *SPIC Energy Science and Technology Research Institute*, Shanghai, China, August 2025.
- **"PANORAMA: Exploring the Industrial Potentials of Omnidirectional Vision"**  
  *Yangtze River Delta International Talent Port*, Wuxi, China, August 2025.
- **"Retrieval-augmented Realistic Image Generation via Self-reflective Contrastive Learning"**  
  *VIVO*, August 2025. Invited talk by [Dr. Kanzhi Wu](https://scholar.google.com.hk/citations?user=N0WHQ2wAAAAJ&hl=zh-CN&oi=ao), Shenzhen, China, August 2025.

---

### Mentorship

**Current:** [Yuanhuiyi Lyu (PhD, HKUST-GZ)](https://qc-ly.github.io/); [Lutao Jiang (PhD, HKUST-GZ)](https://lutao2021.github.io/); [Jialei Chen (PhD, Nagoya)](https://psmobile.github.io/); Mengzhen Chi (PhD, NEU); [Zihao Dongfang (RA, HKUST-GZ)](https://scholar.google.com.hk/citations?hl=zh-CN&user=IvJ4_xsAAAAJ); [Chenfei Liao (MPhil, HKUST-GZ)](https://chenfei-liao.github.io/); Junha Moon (MPhil, HKUST-GZ); [Ziqiao Weng (UG, SCU)](https://katie312.github.io/); Yulong Guo (MS, ZJU); Kaiyu Lei (UG, XJTU); Zhenquan Zhang (MPhil, SCUT); [Boyuan Zheng (MPhil, Tongji)](https://nathandrake67.github.io/zhengby.github.io/); Leyi Sheng (UG, HKUST-GZ)

**Past:** Ding Zhong (MS, Michigan); Zhengxuan Jiang (MPhil, ZJU); Yunhao Luo (PhD, Umich); Tianbo Pan (PhD, NUS); Zijie Lin (MS, USTC)

‚úâÔ∏è <strong>Feel free to contact me for discussion and collaboration!</strong>

---

### Services

{% include_relative _includes/services.md %}


<script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=080808&w=a&t=tt&d=zrl7WjzBxF_qKC05N5OneNhjFigQ9jPab4GJHSWvjkI&co=ffffff&cmo=3acc3a&cmn=ff5353&ct=808080'></script>
